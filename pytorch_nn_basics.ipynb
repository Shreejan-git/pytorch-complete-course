{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOCsIQADnJYBMxXKeycHqUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shreejan-git/pytorch-complete-course/blob/main/pytorch_nn_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "G_45BzF8pWvh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2x33C50qHKQ",
        "outputId": "71918909-ea68-4aee-8377-98cbfcc1dc79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfkKfwe0qJ5y",
        "outputId": "ab519a75-bbfe-4fa9-8e8f-888cb9b812f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes while creating a custom model:**\n",
        "\n",
        "\n",
        "1.   If we are creating our custom neural network module, it must inherit from nn.Module. https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
        "2.   Always call the __ init __ (constructor) of the parent class using the super().__ init__ (). This is a mandatory step. If we do not call the parents's constructor, we will have problems while training and saving the model. (linked with point 3)\n",
        "3. When we are inheriting from nn.Module, it is mandatory to add forward method. Later while training the model, setting our to model to traning model with model.train(X_train), it will automatically forward_pass using forward method.\n",
        "4. Below, nn.parameter's work is to mark that variable as a learnable parameter. nn.parameter will track it while training.\n"
      ],
      "metadata": {
        "id": "2NEBehP6GspC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLinearRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weight = nn.Parameter(torch.rand(1,\n",
        "                               requires_grad=True,\n",
        "                               dtype=torch.float32))\n",
        "    self.bias = nn.Parameter(torch.rand(1,\n",
        "                             requires_grad=True,\n",
        "                             dtype=torch.float32))\n",
        "\n",
        "  def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "    return self.weight * X + self.bias"
      ],
      "metadata": {
        "id": "Ef-umtmWqpIj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleLinearRegression() # Initializing the model"
      ],
      "metadata": {
        "id": "NMaTWt8ZjFig"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resources for loss and optimization functions**\n",
        "\n",
        "Pytorch Loss function: https://pytorch.org/docs/stable/nn.html#loss-functions , https://neptune.ai/blog/pytorch-loss-functions\n",
        "\n",
        "Pytorch optimization functions: https://pytorch.org/docs/stable/optim.html"
      ],
      "metadata": {
        "id": "D-wtoIGyGDN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the loss function.\n",
        "loss_fun = nn.L1Loss() # L1Loss is MAE loss function.\n",
        "print(loss_fun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKPB2zTeFnGe",
        "outputId": "5dc71c26-58f3-45ba-aeb4-8e5c0cb9f5a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1Loss()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can define any optimization function. like Adam, SGD, Adagrad.\n",
        "\n",
        "1st parameter of SGD is the model's parameters. We can get this by writing model.parameters().\n",
        "\n",
        "2nd parameter is learning rate (lr). Higher the value, the bigger the steps will be while reducing the gradient to global minima and vice versa."
      ],
      "metadata": {
        "id": "2eIyUX_QiJ3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the optimization function.\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr = 0.001,)"
      ],
      "metadata": {
        "id": "DgUWd_kCF2Op",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "4021b71b-fb73-4ecd-94da-55c732814f11"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8f26aff175b7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 1st parameter of SGD is the model's parameters. We can get this by writing model.parameters().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Give the exact name of the variable which we named while initializing the Class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m optimizer = torch.optim.SGD(params=model.parameters(),\n\u001b[0m\u001b[1;32m      6\u001b[0m                             lr = 0.001,)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting the training and evaluation loop**"
      ],
      "metadata": {
        "id": "kGm13Jybi25z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1"
      ],
      "metadata": {
        "id": "_o0RQkLCi9lM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Below code explanation:**\n",
        "\n",
        "\n",
        "1.   Line 3: model.train() This line sets the PyTorch model (model) to training mode. In training mode, the model keeps track of the operations and layers that require gradients for backpropagation. This is necessary because some layers, like dropout or batch normalization(if we have one) must be turned on while training and turned off while evaluating.\n",
        "2.   Line 6: y_pred = model(X_train) This line performs a forward pass through the model using the training data X_train. It computes the predicted values (y_pred) based on the current model parameters. This step is a fundamental part of training, as it computes the model's predictions.\n",
        "3. line 12 [IMPORTANT] In PyTorch, when you perform a backward pass (loss.backward()), the gradients of the model's parameters are accumulated in the respective parameter tensors. But, when you're training a neural network with a single loss function and a single backward pass per iteration, you typically want to reset the gradients to zero before each backward pass to avoid accumulating gradients from previous iterations. Without zeroing out gradients, if you run multiple training iterations, the gradients will accumulate over time. This can lead to incorrect gradient updates, making your model's training unstable or divergent. By using optimizer.zero_grad() before each backward pass, you ensure that the gradients are cleared, and only the gradients of the current iteration are used to update the model parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "mJM-zCi5jZvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    # Set the model to traning mode\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass\n",
        "    y_pred = model(X_train)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = loss_fn(y_pred, y_train)\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform backpropagation on the loss with respect to the parameters of the model\n",
        "    # Parameters are those which we set requires_grad = True.\n",
        "    loss.backward()\n",
        "\n",
        "    # step the optimizer (perform gradient descent)\n",
        "    optimizer.step()\n",
        "\n",
        "    # testing\n",
        "    model.eval()"
      ],
      "metadata": {
        "id": "ShDAp3vk8_W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XUI_OG1w9AyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KRxrr3VB9BT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7okZTnjs9BpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "grvpgRv09B-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MvZikrqeqosG"
      }
    }
  ]
}